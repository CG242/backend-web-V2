from celery import shared_task
from django.utils import timezone
from datetime import timedelta, datetime
import random
import logging
from .models import (
    Capteur, Mesure, Zone, DonneesEnvironnementales, AnalyseErosion,
    CapteurArduino, MesureArduino, DonneesManquantes, LogCapteurArduino,
    EvenementExterne, FusionDonnees, PredictionEnrichie, AlerteEnrichie, ArchiveDonnees
)
# Imports supprim√©s - fichiers de services inutilis√©s supprim√©s
from .services.analyse_fusion_service import AnalyseFusionService, ArchiveService

logger = logging.getLogger(__name__)


@shared_task
def generer_mesures_automatiques():
    """
    T√¢che Celery pour g√©n√©rer automatiquement des mesures
    Ex√©cut√©e toutes les X minutes selon la fr√©quence des capteurs
    """
    print("üîÑ G√©n√©ration automatique de mesures...")
    
    capteurs_actifs = Capteur.objects.filter(etat='actif')
    mesures_creees = 0
    
    for capteur in capteurs_actifs:
        # V√©rifier si le capteur doit prendre une mesure maintenant
        derniere_mesure = capteur.mesures.first()
        if derniere_mesure:
            temps_ecoule = timezone.now() - derniere_mesure.timestamp
            frequence_minutes = timedelta(minutes=capteur.frequence_mesure_min)
            
            if temps_ecoule < frequence_minutes:
                continue  # Pas encore le moment de prendre une mesure
        
        # G√©n√©rer une nouvelle mesure
        valeur = generer_valeur_mesure(capteur.type)
        unite = get_unite_mesure(capteur.type)
        
        Mesure.objects.create(
            capteur=capteur,
            valeur=valeur,
            unite=unite,
            timestamp=timezone.now(),
            qualite_donnee='bonne',
            commentaires="Mesure automatique g√©n√©r√©e"
        )
        mesures_creees += 1
    
    print(f"‚úÖ {mesures_creees} mesures g√©n√©r√©es automatiquement")
    return f"{mesures_creees} mesures cr√©√©es"


def generer_valeur_mesure(type_capteur):
    """G√©n√®re une valeur r√©aliste selon le type de capteur"""
    if type_capteur == "temperature":
        return round(random.uniform(24, 34), 1)
    elif type_capteur == "salinite":
        return round(random.uniform(30, 40), 2)
    elif type_capteur == "houle":
        return round(random.uniform(0.5, 3.5), 2)
    elif type_capteur == "vent":
        return round(random.uniform(5, 60), 1)
    elif type_capteur == "pluviometrie":
        return round(random.uniform(0, 100), 1)
    elif type_capteur == "niveau_mer":
        return round(random.uniform(-2, 4), 2)
    elif type_capteur == "ph":
        return round(random.uniform(7.5, 8.5), 2)
    elif type_capteur == "turbidite":
        return round(random.uniform(0.1, 50), 1)
    else:
        return round(random.uniform(0, 100), 2)


def get_unite_mesure(type_capteur):
    """Retourne l'unit√© de mesure selon le type de capteur"""
    unites = {
        'temperature': '¬∞C',
        'salinite': 'PSU',
        'houle': 'm',
        'vent': 'km/h',
        'pluviometrie': 'mm',
        'niveau_mer': 'm',
        'ph': 'pH',
        'turbidite': 'NTU'
    }
    return unites.get(type_capteur, 'unit')


@shared_task
def nettoyer_anciennes_mesures():
    """
    T√¢che pour nettoyer les anciennes mesures (plus de 1 an)
    """
    print("üßπ Nettoyage des anciennes mesures...")
    
    date_limite = timezone.now() - timedelta(days=365)
    anciennes_mesures = Mesure.objects.filter(timestamp__lt=date_limite)
    nombre_supprimees = anciennes_mesures.count()
    
    anciennes_mesures.delete()
    
    print(f"‚úÖ {nombre_supprimees} anciennes mesures supprim√©es")
    return f"{nombre_supprimees} mesures supprim√©es"


@shared_task
def verifier_etat_capteurs():
    """
    T√¢che pour v√©rifier l'√©tat des capteurs et g√©n√©rer des alertes
    """
    print("üîç V√©rification de l'√©tat des capteurs...")
    
    capteurs_defaillants = []
    
    for capteur in Capteur.objects.filter(etat='actif'):
        # V√©rifier si le capteur n'a pas envoy√© de donn√©es r√©cemment
        derniere_mesure = capteur.mesures.first()
        if derniere_mesure:
            temps_ecoule = timezone.now() - derniere_mesure.timestamp
            # Si pas de mesure depuis plus de 2x la fr√©quence normale
            frequence_max = timedelta(minutes=capteur.frequence_mesure_min * 2)
            
            if temps_ecoule > frequence_max:
                capteur.etat = 'defaillant'
                capteur.save()
                capteurs_defaillants.append(capteur.nom)
    
    print(f"‚ö†Ô∏è {len(capteurs_defaillants)} capteurs marqu√©s comme d√©faillants")
    return f"{len(capteurs_defaillants)} capteurs d√©faillants d√©tect√©s"


# ============================================================================
# NOUVELLES T√ÇCHES POUR LES FONCTIONNALIT√âS ENRICHIES
# ============================================================================

@shared_task
def collecter_donnees_environnementales():
    """
    T√¢che pour collecter automatiquement les donn√©es environnementales
    de toutes les zones actives
    """
    print("üåç Collecte automatique des donn√©es environnementales...")
    
    zones_actives = Zone.objects.all()
    donnees_collectees = 0
    
    for zone in zones_actives:
        try:
            # D√©finir la p√©riode de collecte (derni√®res 24h)
            end_date = datetime.now()
            start_date = end_date - timedelta(days=1)
            
            # Formater les dates pour les APIs
            start_date_str = start_date.strftime('%Y-%m-%d')
            end_date_str = end_date.strftime('%Y-%m-%d')
            
            # Collecter les donn√©es
            consolidation_service = DataConsolidationService()
            consolidated_data = consolidation_service.collect_all_data(
                zone, start_date_str, end_date_str
            )
            
            # Sauvegarder les donn√©es
            donnees_env = consolidation_service.save_consolidated_data(zone, consolidated_data)
            donnees_collectees += 1
            
            print(f"‚úÖ Donn√©es collect√©es pour {zone.nom}")
            
        except Exception as e:
            print(f"‚ùå Erreur collecte {zone.nom}: {e}")
    
    print(f"üìä {donnees_collectees} zones trait√©es")
    return f"{donnees_collectees} zones trait√©es"


@shared_task
def generer_analyses_erosion_automatiques():
    """
    T√¢che pour g√©n√©rer automatiquement des analyses d'√©rosion
    pour toutes les zones avec des donn√©es environnementales r√©centes
    """
    print("üî¨ G√©n√©ration automatique d'analyses d'√©rosion...")
    
    # R√©cup√©rer les zones avec des donn√©es environnementales r√©centes
    zones_avec_donnees = Zone.objects.filter(
        donnees_environnementales__date_collecte__gte=timezone.now() - timedelta(days=1)
    ).distinct()
    
    analyses_creees = 0
    
    for zone in zones_avec_donnees:
        try:
            # R√©cup√©rer les donn√©es environnementales les plus r√©centes
            donnees_env = DonneesEnvironnementales.objects.filter(
                zone=zone
            ).order_by('-date_collecte').first()
            
            if donnees_env:
                # V√©rifier si une analyse r√©cente existe d√©j√†
                analyse_recente = AnalyseErosion.objects.filter(
                    zone=zone,
                    donnees_environnementales=donnees_env,
                    date_analyse__gte=timezone.now() - timedelta(hours=6)
                ).exists()
                
                if not analyse_recente:
                    # Cr√©er une nouvelle analyse
                    analyse_view = AnalyseErosionViewSet()
                    analyse = analyse_view._calculer_analyse_erosion(zone, donnees_env, 30)
                    analyses_creees += 1
                    
                    print(f"‚úÖ Analyse cr√©√©e pour {zone.nom}")
            
        except Exception as e:
            print(f"‚ùå Erreur analyse {zone.nom}: {e}")
    
    print(f"üìà {analyses_creees} analyses cr√©√©es")
    return f"{analyses_creees} analyses cr√©√©es"


@shared_task
def nettoyer_donnees_anciennes():
    """
    T√¢che pour nettoyer les anciennes donn√©es environnementales et analyses
    """
    print("üßπ Nettoyage des anciennes donn√©es...")
    
    # Nettoyer les donn√©es environnementales anciennes (plus de 3 mois)
    date_limite_env = timezone.now() - timedelta(days=90)
    anciennes_donnees_env = DonneesEnvironnementales.objects.filter(
        date_collecte__lt=date_limite_env
    )
    nb_env_supprimees = anciennes_donnees_env.count()
    anciennes_donnees_env.delete()
    
    # Nettoyer les analyses anciennes (plus de 6 mois)
    date_limite_analyses = timezone.now() - timedelta(days=180)
    anciennes_analyses = AnalyseErosion.objects.filter(
        date_analyse__lt=date_limite_analyses
    )
    nb_analyses_supprimees = anciennes_analyses.count()
    anciennes_analyses.delete()
    
    print(f"‚úÖ {nb_env_supprimees} donn√©es environnementales supprim√©es")
    print(f"‚úÖ {nb_analyses_supprimees} analyses supprim√©es")
    
    return f"{nb_env_supprimees} donn√©es env et {nb_analyses_supprimees} analyses supprim√©es"


@shared_task
def synchroniser_donnees_cartographiques():
    """
    T√¢che pour synchroniser les donn√©es cartographiques avec les APIs externes
    """
    print("üó∫Ô∏è Synchronisation des donn√©es cartographiques...")
    
    # Cette t√¢che pourrait √™tre √©tendue pour t√©l√©charger automatiquement
    # les nouvelles images satellites, donn√©es de substrat, etc.
    
    zones_actives = Zone.objects.all()
    donnees_synchronisees = 0
    
    for zone in zones_actives:
        try:
            # Ici, on pourrait impl√©menter la logique pour :
            # - T√©l√©charger les nouvelles images satellites
            # - Mettre √† jour les donn√©es de substrat
            # - Synchroniser les donn√©es hydrographiques
            
            print(f"‚úÖ Donn√©es cartographiques synchronis√©es pour {zone.nom}")
            donnees_synchronisees += 1
            
        except Exception as e:
            print(f"‚ùå Erreur synchronisation {zone.nom}: {e}")
    
    print(f"üó∫Ô∏è {donnees_synchronisees} zones synchronis√©es")
    return f"{donnees_synchronisees} zones synchronis√©es"


@shared_task
def generer_rapport_quotidien():
    """
    T√¢che pour g√©n√©rer un rapport quotidien des activit√©s du syst√®me
    """
    print("üìä G√©n√©ration du rapport quotidien...")
    
    # Statistiques du jour
    aujourd_hui = timezone.now().date()
    
    # Nombre de mesures g√©n√©r√©es
    mesures_aujourd_hui = Mesure.objects.filter(
        timestamp__date=aujourd_hui
    ).count()
    
    # Nombre de donn√©es environnementales collect√©es
    donnees_env_aujourd_hui = DonneesEnvironnementales.objects.filter(
        date_collecte__date=aujourd_hui
    ).count()
    
    # Nombre d'analyses cr√©√©es
    analyses_aujourd_hui = AnalyseErosion.objects.filter(
        date_analyse__date=aujourd_hui
    ).count()
    
    # Zones actives
    zones_actives = Zone.objects.count()
    
    rapport = {
        'date': aujourd_hui.isoformat(),
        'mesures_generes': mesures_aujourd_hui,
        'donnees_environnementales': donnees_env_aujourd_hui,
        'analyses_erosion': analyses_aujourd_hui,
        'zones_actives': zones_actives,
        'statut_systeme': 'op√©rationnel'
    }
    
    print(f"üìà Rapport quotidien g√©n√©r√©: {rapport}")
    return rapport


# ============================================================================
# NOUVELLES T√ÇCHES POUR LES CAPTEURS ARDUINO
# ============================================================================

# T√¢ches supprim√©es - services Arduino inutilis√©s supprim√©s
# @shared_task
# def monitorer_capteurs_arduino():
#     """T√¢che supprim√©e - service ArduinoMonitoringService supprim√©"""
#     pass

# @shared_task
# def detecter_donnees_manquantes_arduino():
#     """T√¢che supprim√©e - service DataCompletionService supprim√©"""
#     pass

# @shared_task
# def completer_donnees_manquantes_arduino():
#     """T√¢che supprim√©e - service DataCompletionService supprim√©"""
#     pass

# @shared_task
# def simuler_donnees_arduino_test():
#     """T√¢che supprim√©e - service ArduinoConnectionService supprim√©"""
#     pass

# @shared_task
# def detecter_capteurs_automatique():
#     """T√¢che supprim√©e - service CapteurDetectionService supprim√©"""
#     pass

# @shared_task
# def envoyer_notifications_quotidiennes():
#     """T√¢che supprim√©e - service CapteurNotificationService supprim√©"""
#     pass


# ============================================================================
# NOUVELLES T√ÇCHES POUR √âV√âNEMENTS EXTERNES ET FUSION DE DONN√âES
# ============================================================================

@shared_task
def analyser_fusion_evenement(evenement_id: int):
    """
    T√¢che pour analyser un √©v√©nement externe et cr√©er une fusion de donn√©es
    """
    logger.info(f"Analyse de fusion pour l'√©v√©nement {evenement_id}")
    
    try:
        service = AnalyseFusionService()
        resultat = service.analyser_evenement(evenement_id)
        
        if resultat['success']:
            logger.info(f"Analyse termin√©e pour l'√©v√©nement {evenement_id}: {resultat['message']}")
            return f"Analyse r√©ussie: {resultat['message']}"
        else:
            logger.error(f"√âchec analyse √©v√©nement {evenement_id}: {resultat['message']}")
            return f"√âchec: {resultat['message']}"
            
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse de l'√©v√©nement {evenement_id}: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def analyser_fusion_zone(zone_id: int, periode_jours: int = 30):
    """
    T√¢che pour analyser une zone compl√®te et cr√©er des fusions de donn√©es
    """
    logger.info(f"Analyse de fusion pour la zone {zone_id} sur {periode_jours} jours")
    
    try:
        service = AnalyseFusionService()
        resultat = service.analyser_zone(zone_id, periode_jours)
        
        if resultat['success']:
            logger.info(f"Analyse de zone termin√©e: {resultat['message']}")
            return f"Analyse r√©ussie: {resultat['message']}"
        else:
            logger.error(f"√âchec analyse zone {zone_id}: {resultat['message']}")
            return f"√âchec: {resultat['message']}"
            
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse de la zone {zone_id}: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def traiter_evenements_en_attente():
    """
    T√¢che pour traiter les √©v√©nements externes en attente de traitement
    """
    logger.info("Traitement des √©v√©nements en attente")
    
    try:
        # R√©cup√©rer les √©v√©nements non trait√©s des derni√®res 24h
        date_limite = timezone.now() - timedelta(hours=24)
        evenements_en_attente = EvenementExterne.objects.filter(
            is_traite=False,
            is_valide=True,
            date_evenement__gte=date_limite
        ).order_by('date_evenement')
        
        evenements_traites = 0
        
        for evenement in evenements_en_attente:
            try:
                # Marquer comme trait√© et d√©clencher l'analyse
                evenement.is_traite = True
                evenement.save()
                
                # D√©clencher l'analyse
                analyser_fusion_evenement.delay(evenement.id)
                evenements_traites += 1
                
            except Exception as e:
                logger.error(f"Erreur traitement √©v√©nement {evenement.id}: {e}")
        
        logger.info(f"Traitement termin√©: {evenements_traites} √©v√©nements trait√©s")
        return f"{evenements_traites} √©v√©nements trait√©s"
        
    except Exception as e:
        logger.error(f"Erreur lors du traitement des √©v√©nements en attente: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def nettoyer_anciens_evenements():
    """
    T√¢che pour nettoyer les anciens √©v√©nements externes
    """
    logger.info("Nettoyage des anciens √©v√©nements externes")
    
    try:
        # Supprimer les √©v√©nements de simulation de plus de 30 jours
        date_limite_simulation = timezone.now() - timedelta(days=30)
        anciens_simulations = EvenementExterne.objects.filter(
            is_simulation=True,
            date_evenement__lt=date_limite_simulation
        )
        nb_simulations_supprimees = anciens_simulations.count()
        anciens_simulations.delete()
        
        # Supprimer les √©v√©nements invalides de plus de 7 jours
        date_limite_invalides = timezone.now() - timedelta(days=7)
        anciens_invalides = EvenementExterne.objects.filter(
            is_valide=False,
            date_evenement__lt=date_limite_invalides
        )
        nb_invalides_supprimees = anciens_invalides.count()
        anciens_invalides.delete()
        
        # Supprimer les √©v√©nements trait√©s de plus de 90 jours
        date_limite_traites = timezone.now() - timedelta(days=90)
        anciens_traites = EvenementExterne.objects.filter(
            is_traite=True,
            date_evenement__lt=date_limite_traites
        )
        nb_traites_supprimees = anciens_traites.count()
        anciens_traites.delete()
        
        logger.info(f"Nettoyage termin√©: {nb_simulations_supprimees} simulations, "
                   f"{nb_invalides_supprimees} invalides, {nb_traites_supprimees} trait√©s supprim√©s")
        
        return f"{nb_simulations_supprimees + nb_invalides_supprimees + nb_traites_supprimees} √©v√©nements supprim√©s"
        
    except Exception as e:
        logger.error(f"Erreur lors du nettoyage des √©v√©nements: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def nettoyer_anciennes_fusions():
    """
    T√¢che pour nettoyer les anciennes fusions de donn√©es
    """
    logger.info("Nettoyage des anciennes fusions de donn√©es")
    
    try:
        # Supprimer les fusions termin√©es de plus de 6 mois
        date_limite = timezone.now() - timedelta(days=180)
        anciennes_fusions = FusionDonnees.objects.filter(
            statut='terminee',
            date_creation__lt=date_limite
        )
        nb_fusions_supprimees = anciennes_fusions.count()
        anciennes_fusions.delete()
        
        # Supprimer les fusions en erreur de plus de 30 jours
        date_limite_erreurs = timezone.now() - timedelta(days=30)
        anciennes_erreurs = FusionDonnees.objects.filter(
            statut='erreur',
            date_creation__lt=date_limite_erreurs
        )
        nb_erreurs_supprimees = anciennes_erreurs.count()
        anciennes_erreurs.delete()
        
        logger.info(f"Nettoyage fusions termin√©: {nb_fusions_supprimees} fusions, "
                   f"{nb_erreurs_supprimees} erreurs supprim√©es")
        
        return f"{nb_fusions_supprimees + nb_erreurs_supprimees} fusions supprim√©es"
        
    except Exception as e:
        logger.error(f"Erreur lors du nettoyage des fusions: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def nettoyer_anciennes_predictions():
    """
    T√¢che pour nettoyer les anciennes pr√©dictions enrichies
    """
    logger.info("Nettoyage des anciennes pr√©dictions enrichies")
    
    try:
        # Supprimer les pr√©dictions de plus de 1 an
        date_limite = timezone.now() - timedelta(days=365)
        anciennes_predictions = PredictionEnrichie.objects.filter(
            date_prediction__lt=date_limite
        )
        nb_predictions_supprimees = anciennes_predictions.count()
        anciennes_predictions.delete()
        
        logger.info(f"Nettoyage pr√©dictions termin√©: {nb_predictions_supprimees} pr√©dictions supprim√©es")
        return f"{nb_predictions_supprimees} pr√©dictions supprim√©es"
        
    except Exception as e:
        logger.error(f"Erreur lors du nettoyage des pr√©dictions: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def nettoyer_anciennes_alertes():
    """
    T√¢che pour nettoyer les anciennes alertes enrichies
    """
    logger.info("Nettoyage des anciennes alertes enrichies")
    
    try:
        # R√©soudre les alertes anciennes non r√©solues
        date_limite_resolution = timezone.now() - timedelta(days=30)
        alertes_anciennes = AlerteEnrichie.objects.filter(
            est_resolue=False,
            date_creation__lt=date_limite_resolution
        )
        nb_alertes_resolues = alertes_anciennes.count()
        
        for alerte in alertes_anciennes:
            alerte.est_resolue = True
            alerte.est_active = False
            alerte.date_resolution = timezone.now()
            alerte.commentaires += " R√©solue automatiquement par nettoyage."
            alerte.save()
        
        # Supprimer les alertes r√©solues de plus de 6 mois
        date_limite_suppression = timezone.now() - timedelta(days=180)
        anciennes_alertes = AlerteEnrichie.objects.filter(
            est_resolue=True,
            date_resolution__lt=date_limite_suppression
        )
        nb_alertes_supprimees = anciennes_alertes.count()
        anciennes_alertes.delete()
        
        logger.info(f"Nettoyage alertes termin√©: {nb_alertes_resolues} r√©solues, "
                   f"{nb_alertes_supprimees} supprim√©es")
        
        return f"{nb_alertes_resolues} alertes r√©solues, {nb_alertes_supprimees} supprim√©es"
        
    except Exception as e:
        logger.error(f"Erreur lors du nettoyage des alertes: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def creer_archive_donnees(type_donnees: str, zone_id: int, periode_jours: int):
    """
    T√¢che pour cr√©er une archive de donn√©es
    """
    logger.info(f"Cr√©ation d'archive {type_donnees} pour la zone {zone_id}")
    
    try:
        service = ArchiveService()
        resultat = service.creer_archive(type_donnees, zone_id, periode_jours)
        
        if resultat['success']:
            logger.info(f"Archive cr√©√©e: {resultat['archive_id']}")
            return f"Archive cr√©√©e: {resultat['nombre_elements']} √©l√©ments"
        else:
            logger.error(f"√âchec cr√©ation archive: {resultat['message']}")
            return f"√âchec: {resultat['message']}"
            
    except Exception as e:
        logger.error(f"Erreur lors de la cr√©ation de l'archive: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def purger_anciennes_archives(periode_jours: int = 365):
    """
    T√¢che pour purger les anciennes archives
    """
    logger.info(f"Purge des archives de plus de {periode_jours} jours")
    
    try:
        date_limite = timezone.now() - timedelta(days=periode_jours)
        anciennes_archives = ArchiveDonnees.objects.filter(
            date_archivage__lt=date_limite,
            est_disponible=True
        )
        
        nb_archives_supprimees = 0
        
        for archive in anciennes_archives:
            try:
                # Supprimer le fichier physique
                import os
                if os.path.exists(archive.chemin_fichier):
                    os.remove(archive.chemin_fichier)
                
                # Marquer comme supprim√©e
                archive.est_disponible = False
                archive.date_suppression = timezone.now()
                archive.save()
                
                nb_archives_supprimees += 1
                
            except Exception as e:
                logger.error(f"Erreur suppression archive {archive.id}: {e}")
        
        logger.info(f"Purge termin√©e: {nb_archives_supprimees} archives supprim√©es")
        return f"{nb_archives_supprimees} archives supprim√©es"
        
    except Exception as e:
        logger.error(f"Erreur lors de la purge des archives: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def generer_rapport_fusion_quotidien():
    """
    T√¢che pour g√©n√©rer un rapport quotidien de fusion des donn√©es
    """
    logger.info("G√©n√©ration du rapport quotidien de fusion")
    
    try:
        aujourd_hui = timezone.now().date()
        hier = aujourd_hui - timedelta(days=1)
        
        # Statistiques des √©v√©nements
        evenements_hier = EvenementExterne.objects.filter(
            date_evenement__date=hier
        )
        
        # Statistiques des fusions
        fusions_hier = FusionDonnees.objects.filter(
            date_creation__date=hier
        )
        
        # Statistiques des pr√©dictions
        predictions_hier = PredictionEnrichie.objects.filter(
            date_prediction__date=hier
        )
        
        # Statistiques des alertes
        alertes_hier = AlerteEnrichie.objects.filter(
            date_creation__date=hier
        )
        
        rapport = {
            'date': hier.isoformat(),
            'evenements': {
                'total': evenements_hier.count(),
                'traites': evenements_hier.filter(is_traite=True).count(),
                'non_traites': evenements_hier.filter(is_traite=False).count(),
                'simulations': evenements_hier.filter(is_simulation=True).count(),
                'par_type': dict(evenements_hier.values_list('type_evenement').annotate(count=Count('id')))
            },
            'fusions': {
                'total': fusions_hier.count(),
                'terminees': fusions_hier.filter(statut='terminee').count(),
                'en_cours': fusions_hier.filter(statut='en_cours').count(),
                'erreurs': fusions_hier.filter(statut='erreur').count()
            },
            'predictions': {
                'total': predictions_hier.count(),
                'erosion_predite': predictions_hier.filter(erosion_predite=True).count(),
                'erosion_non_predite': predictions_hier.filter(erosion_predite=False).count(),
                'par_niveau': dict(predictions_hier.values_list('niveau_erosion').annotate(count=Count('id')))
            },
            'alertes': {
                'total': alertes_hier.count(),
                'actives': alertes_hier.filter(est_active=True).count(),
                'resolues': alertes_hier.filter(est_resolue=True).count(),
                'par_niveau': dict(alertes_hier.values_list('niveau').annotate(count=Count('id')))
            },
            'statut_systeme': 'op√©rationnel'
        }
        
        logger.info(f"Rapport quotidien g√©n√©r√©: {rapport}")
        return rapport
        
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration du rapport quotidien: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def exporter_donnees_ia():
    """
    T√¢che pour exporter les donn√©es pour l'IA (format ML)
    """
    logger.info("Export des donn√©es pour l'IA")
    
    try:
        # R√©cup√©rer les donn√©es des 6 derniers mois
        date_limite = timezone.now() - timedelta(days=180)
        
        # Donn√©es d'entra√Ænement
        donnees_entrainement = {
            'evenements': [],
            'mesures_arduino': [],
            'fusions': [],
            'predictions': []
        }
        
        # √âv√©nements
        evenements = EvenementExterne.objects.filter(
            date_evenement__gte=date_limite,
            is_valide=True
        )
        
        for e in evenements:
            donnees_entrainement['evenements'].append({
                'type_evenement': e.type_evenement,
                'intensite': e.intensite,
                'zone_id': e.zone.id,
                'date_evenement': e.date_evenement.isoformat(),
                'duree_minutes': e.duree_minutes or 0,
                'rayon_impact_km': e.rayon_impact_km or 0
            })
        
        # Mesures Arduino
        mesures = MesureArduino.objects.filter(
            timestamp__gte=date_limite,
            est_valide=True
        ).select_related('capteur')
        
        for m in mesures:
            donnees_entrainement['mesures_arduino'].append({
                'capteur_type': m.capteur.type_capteur,
                'valeur': m.valeur,
                'zone_id': m.capteur.zone.id,
                'timestamp': m.timestamp.isoformat(),
                'qualite_donnee': m.qualite_donnee
            })
        
        # Fusions
        fusions = FusionDonnees.objects.filter(
            date_creation__gte=date_limite,
            statut='terminee'
        )
        
        for f in fusions:
            donnees_entrainement['fusions'].append({
                'zone_id': f.zone.id,
                'score_erosion': f.score_erosion,
                'probabilite_erosion': f.probabilite_erosion,
                'mesures_count': f.mesures_arduino_count,
                'evenements_count': f.evenements_externes_count,
                'facteurs_dominants': f.facteurs_dominants
            })
        
        # Pr√©dictions
        predictions = PredictionEnrichie.objects.filter(
            date_prediction__gte=date_limite
        )
        
        for p in predictions:
            donnees_entrainement['predictions'].append({
                'zone_id': p.zone.id,
                'erosion_predite': p.erosion_predite,
                'niveau_erosion': p.niveau_erosion,
                'confiance_pourcentage': p.confiance_pourcentage,
                'taux_erosion_pred': p.taux_erosion_pred_m_an,
                'horizon_jours': p.horizon_jours
            })
        
        # Sauvegarder le fichier d'export
        import json
        import os
        
        chemin_export = f"exports/ia/donnees_entrainement_{timezone.now().strftime('%Y%m%d')}.json"
        os.makedirs(os.path.dirname(chemin_export), exist_ok=True)
        
        with open(chemin_export, 'w', encoding='utf-8') as f:
            json.dump(donnees_entrainement, f, ensure_ascii=False, indent=2)
        
        # Statistiques
        stats = {
            'evenements': len(donnees_entrainement['evenements']),
            'mesures': len(donnees_entrainement['mesures_arduino']),
            'fusions': len(donnees_entrainement['fusions']),
            'predictions': len(donnees_entrainement['predictions']),
            'chemin_fichier': chemin_export,
            'date_export': timezone.now().isoformat()
        }
        
        logger.info(f"Export IA termin√©: {stats}")
        return f"Export r√©ussi: {stats['evenements']} √©v√©nements, {stats['mesures']} mesures, {stats['fusions']} fusions, {stats['predictions']} pr√©dictions"
        
    except Exception as e:
        logger.error(f"Erreur lors de l'export IA: {e}")
        return f"Erreur: {str(e)}"


# ============================================================================
# NOUVELLES T√ÇCHES POUR LES PR√âDICTIONS ML
# ============================================================================

@shared_task
def calculer_predictions_automatiques():
    """
    T√¢che pour calculer automatiquement les pr√©dictions d'√©rosion pour toutes les zones
    Ex√©cut√©e quotidiennement ou sur demande
    """
    logger.info("ü§ñ Calcul automatique des pr√©dictions d'√©rosion")
    
    try:
        from .models import Zone, ModeleML
        from .ml_services import MLPredictionService
        
        # V√©rifier qu'il y a un mod√®le actif
        active_model = ModeleML.objects.filter(statut='actif').first()
        if not active_model:
            logger.warning("Aucun mod√®le ML actif trouv√© pour les pr√©dictions automatiques")
            return "Aucun mod√®le ML actif - pr√©dictions ignor√©es"
        
        # R√©cup√©rer toutes les zones actives
        zones_actives = Zone.objects.all()
        predictions_creees = 0
        erreurs = 0
        
        ml_service = MLPredictionService()
        
        for zone in zones_actives:
            try:
                # V√©rifier si une pr√©diction r√©cente existe d√©j√† (derni√®res 24h)
                from django.utils import timezone
                from datetime import timedelta
                
                derniere_prediction = zone.predictions.filter(
                    date_prediction__gte=timezone.now() - timedelta(hours=24)
                ).first()
                
                if derniere_prediction:
                    logger.info(f"Pr√©diction r√©cente existante pour {zone.nom} - ignor√©e")
                    continue
                
                # Calculer la pr√©diction pour diff√©rents horizons
                horizons = [7, 30, 90]  # 1 semaine, 1 mois, 3 mois
                
                for horizon in horizons:
                    prediction = ml_service.predire_erosion(
                        zone_id=zone.id,
                        features={},  # Pas de features suppl√©mentaires pour les pr√©dictions automatiques
                        horizon_jours=horizon
                    )
                    
                    # Ajouter un commentaire pour identifier les pr√©dictions automatiques
                    prediction.commentaires = f"Pr√©diction automatique g√©n√©r√©e le {timezone.now().strftime('%Y-%m-%d %H:%M')}"
                    prediction.save()
                    
                    predictions_creees += 1
                    logger.info(f"‚úÖ Pr√©diction cr√©√©e pour {zone.nom} (horizon: {horizon}j)")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur pr√©diction {zone.nom}: {e}")
                erreurs += 1
        
        resultat = f"Pr√©dictions automatiques termin√©es: {predictions_creees} cr√©√©es, {erreurs} erreurs"
        logger.info(resultat)
        return resultat
        
    except Exception as e:
        logger.error(f"Erreur lors du calcul des pr√©dictions automatiques: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def calculer_prediction_zone(zone_id: int, horizon_jours: int = 30, features: dict = None):
    """
    T√¢che pour calculer une pr√©diction pour une zone sp√©cifique
    
    Args:
        zone_id: ID de la zone
        horizon_jours: Horizon de pr√©diction en jours
        features: Features suppl√©mentaires (optionnel)
    """
    logger.info(f"üéØ Calcul de pr√©diction pour la zone {zone_id} (horizon: {horizon_jours}j)")
    
    try:
        from .models import Zone
        from .ml_services import MLPredictionService
        
        # V√©rifier que la zone existe
        try:
            zone = Zone.objects.get(id=zone_id)
        except Zone.DoesNotExist:
            logger.error(f"Zone {zone_id} non trouv√©e")
            return f"Zone {zone_id} non trouv√©e"
        
        # Calculer la pr√©diction
        ml_service = MLPredictionService()
        prediction = ml_service.predire_erosion(
            zone_id=zone_id,
            features=features or {},
            horizon_jours=horizon_jours
        )
        
        # Ajouter un commentaire pour identifier les pr√©dictions par t√¢che
        prediction.commentaires = f"Pr√©diction calcul√©e par t√¢che Celery le {timezone.now().strftime('%Y-%m-%d %H:%M')}"
        prediction.save()
        
        resultat = f"Pr√©diction cr√©√©e: ID {prediction.id} pour {zone.nom} - Taux: {prediction.taux_erosion_pred_m_an:.3f} m/an"
        logger.info(resultat)
        return resultat
        
    except Exception as e:
        logger.error(f"Erreur lors du calcul de pr√©diction pour la zone {zone_id}: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def entrainer_modeles_ml():
    """
    T√¢che pour entra√Æner automatiquement les mod√®les ML
    Ex√©cut√©e p√©riodiquement (hebdomadaire) ou sur demande
    """
    logger.info("üß† Entra√Ænement automatique des mod√®les ML")
    
    try:
        from .ml_services import MLTrainingService
        
        # V√©rifier les pr√©requis
        from .models import HistoriqueErosion, Zone
        
        total_zones = Zone.objects.count()
        total_historique = HistoriqueErosion.objects.count()
        
        if total_zones == 0:
            logger.warning("Aucune zone trouv√©e - entra√Ænement ignor√©")
            return "Aucune zone trouv√©e"
        
        if total_historique < 10:
            logger.warning(f"Pas assez de donn√©es historiques ({total_historique}) - entra√Ænement ignor√©")
            return f"Pas assez de donn√©es historiques ({total_historique})"
        
        # Entra√Æner les mod√®les
        training_service = MLTrainingService()
        results = training_service.train_models()
        
        # Analyser les r√©sultats
        if results.get('errors'):
            error_msg = 'Erreurs lors de l\'entra√Ænement:\n' + '\n'.join(results['errors'])
            logger.error(error_msg)
            return f"Erreurs: {error_msg}"
        
        # Compter les mod√®les cr√©√©s
        models_created = 0
        if results.get('random_forest') and 'error' not in results['random_forest']:
            models_created += 1
        if results.get('regression_lineaire') and 'error' not in results['regression_lineaire']:
            models_created += 1
        
        # Trouver le mod√®le actif
        from .models import ModeleML
        active_model = ModeleML.objects.filter(statut='actif').first()
        
        resultat = f"Entra√Ænement termin√©: {models_created} mod√®les cr√©√©s"
        if active_model:
            resultat += f", mod√®le actif: {active_model.nom} v{active_model.version}"
        
        logger.info(resultat)
        return resultat
        
    except Exception as e:
        logger.error(f"Erreur lors de l'entra√Ænement automatique: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def evaluer_performance_modeles():
    """
    T√¢che pour √©valuer la performance des mod√®les ML existants
    """
    logger.info("üìä √âvaluation de la performance des mod√®les ML")
    
    try:
        from .models import ModeleML, Prediction
        
        models_evalues = 0
        rapport_performance = []
        
        for model in ModeleML.objects.filter(statut__in=['actif', 'inactif']):
            try:
                # R√©cup√©rer les pr√©dictions r√©centes (derniers 30 jours)
                date_limite = timezone.now() - timedelta(days=30)
                predictions_recentes = Prediction.objects.filter(
                    modele_ml=model,
                    date_prediction__gte=date_limite
                )
                
                if predictions_recentes.count() < 5:
                    logger.info(f"Pas assez de pr√©dictions r√©centes pour {model.nom} - ignor√©")
                    continue
                
                # Calculer les m√©triques de performance
                from django.db.models import Avg, StdDev
                
                stats = predictions_recentes.aggregate(
                    confiance_moyenne=Avg('confiance_pourcentage'),
                    confiance_ecart_type=StdDev('confiance_pourcentage'),
                    taux_moyen=Avg('taux_erosion_pred_m_an'),
                    nombre_predictions=Count('id')
                )
                
                performance_data = {
                    'model_id': model.id,
                    'model_name': model.nom,
                    'model_version': model.version,
                    'confiance_moyenne': stats['confiance_moyenne'] or 0,
                    'confiance_ecart_type': stats['confiance_ecart_type'] or 0,
                    'taux_moyen': stats['taux_moyen'] or 0,
                    'nombre_predictions': stats['nombre_predictions'],
                    'score_original': model.precision_score or 0
                }
                
                rapport_performance.append(performance_data)
                models_evalues += 1
                
                logger.info(f"‚úÖ Performance √©valu√©e pour {model.nom}: confiance {performance_data['confiance_moyenne']:.1f}%")
                
            except Exception as e:
                logger.error(f"Erreur √©valuation {model.nom}: {e}")
        
        resultat = f"√âvaluation termin√©e: {models_evalues} mod√®les √©valu√©s"
        logger.info(resultat)
        
        # Optionnel: sauvegarder le rapport de performance
        if rapport_performance:
            import json
            import os
            
            chemin_rapport = f"reports/ml_performance_{timezone.now().strftime('%Y%m%d')}.json"
            os.makedirs(os.path.dirname(chemin_rapport), exist_ok=True)
            
            with open(chemin_rapport, 'w', encoding='utf-8') as f:
                json.dump({
                    'date_evaluation': timezone.now().isoformat(),
                    'models_evalues': models_evalues,
                    'performance_data': rapport_performance
                }, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Rapport de performance sauvegard√©: {chemin_rapport}")
        
        return resultat
        
    except Exception as e:
        logger.error(f"Erreur lors de l'√©valuation de performance: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def nettoyer_anciennes_predictions_ml():
    """
    T√¢che pour nettoyer les anciennes pr√©dictions ML
    """
    logger.info("üßπ Nettoyage des anciennes pr√©dictions ML")
    
    try:
        from .models import Prediction
        
        # Supprimer les pr√©dictions de plus de 6 mois
        date_limite = timezone.now() - timedelta(days=180)
        anciennes_predictions = Prediction.objects.filter(
            date_prediction__lt=date_limite
        )
        
        nb_predictions_supprimees = anciennes_predictions.count()
        anciennes_predictions.delete()
        
        # Supprimer les mod√®les inactifs de plus de 1 an
        from .models import ModeleML
        date_limite_modeles = timezone.now() - timedelta(days=365)
        anciens_modeles = ModeleML.objects.filter(
            statut='inactif',
            date_creation__lt=date_limite_modeles
        )
        
        nb_modeles_supprimees = anciens_modeles.count()
        
        # Supprimer les fichiers de mod√®les associ√©s
        import os
        for model in anciens_modeles:
            try:
                if os.path.exists(model.chemin_fichier):
                    os.remove(model.chemin_fichier)
            except Exception as e:
                logger.warning(f"Impossible de supprimer le fichier {model.chemin_fichier}: {e}")
        
        anciens_modeles.delete()
        
        resultat = f"Nettoyage termin√©: {nb_predictions_supprimees} pr√©dictions, {nb_modeles_supprimees} mod√®les supprim√©s"
        logger.info(resultat)
        return resultat
        
    except Exception as e:
        logger.error(f"Erreur lors du nettoyage des pr√©dictions ML: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def generer_rapport_ml_quotidien():
    """
    T√¢che pour g√©n√©rer un rapport quotidien des activit√©s ML
    """
    logger.info("üìà G√©n√©ration du rapport quotidien ML")
    
    try:
        from .models import ModeleML, Prediction
        
        aujourd_hui = timezone.now().date()
        hier = aujourd_hui - timedelta(days=1)
        
        # Statistiques des pr√©dictions
        predictions_hier = Prediction.objects.filter(
            date_prediction__date=hier
        )
        
        # Statistiques des mod√®les
        modeles_actifs = ModeleML.objects.filter(statut='actif').count()
        modeles_total = ModeleML.objects.count()
        
        # Statistiques par mod√®le
        stats_par_modele = []
        for model in ModeleML.objects.filter(statut='actif'):
            pred_model = predictions_hier.filter(modele_ml=model)
            stats_par_modele.append({
                'model_name': model.nom,
                'model_version': model.version,
                'predictions_count': pred_model.count(),
                'confiance_moyenne': pred_model.aggregate(avg=Avg('confiance_pourcentage'))['avg'] or 0,
                'taux_moyen': pred_model.aggregate(avg=Avg('taux_erosion_pred_m_an'))['avg'] or 0
            })
        
        rapport = {
            'date': hier.isoformat(),
            'predictions': {
                'total': predictions_hier.count(),
                'par_horizon': dict(predictions_hier.values_list('horizon_jours').annotate(count=Count('id'))),
                'confiance_moyenne': predictions_hier.aggregate(avg=Avg('confiance_pourcentage'))['avg'] or 0,
                'taux_moyen': predictions_hier.aggregate(avg=Avg('taux_erosion_pred_m_an'))['avg'] or 0
            },
            'modeles': {
                'actifs': modeles_actifs,
                'total': modeles_total,
                'stats_par_modele': stats_par_modele
            },
            'statut_systeme': 'op√©rationnel'
        }
        
        logger.info(f"Rapport ML quotidien g√©n√©r√©: {rapport}")
        return rapport
        
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration du rapport ML quotidien: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def analyser_evenement_externe(evenement_id):
    """
    T√¢che Celery pour analyser un √©v√©nement externe re√ßu selon le format de votre ami
    """
    logger.info(f"üåä Analyse de l'√©v√©nement externe ID: {evenement_id}")
    
    try:
        evenement = EvenementExterne.objects.get(id=evenement_id)
        
        # Marquer comme en cours de traitement
        evenement.is_traite = False
        evenement.save()
        
        # V√©rifier si l'√©v√©nement n√©cessite une alerte
        if evenement.necessite_alerte:
            alerte = AlerteEnrichie.objects.create(
                zone=evenement.zone,
                type='evenement_extreme',
                niveau='critique' if evenement.niveau_risque == 'critique' else 'alerte',
                titre=f"√âv√©nement climatique critique: {evenement.get_type_evenement_display()}",
                description=f"√âv√©nement {evenement.niveau_risque} d√©tect√©: {evenement.type_evenement} - Intensit√© {evenement.intensite}",
                est_active=True,
                est_resolue=False,
                actions_requises=[
                    "Surveillance renforc√©e de la zone",
                    "√âvaluation des risques d'√©rosion",
                    "Pr√©paration aux mesures d'urgence si n√©cessaire"
                ],
                donnees_contexte={
                    'type_source': 'evenement_externe',
                    'evenement_id': evenement.id,
                    'niveau_risque': evenement.niveau_risque,
                    'zone_erosion': evenement.zone_erosion,
                    'type_evenement': evenement.type_evenement,
                    'intensite': evenement.intensite,
                    'duree': evenement.duree
                }
            )
            
            logger.info(f"Alerte cr√©√©e pour l'√©v√©nement externe: {alerte.id}")
        
        # Marquer comme trait√©
        evenement.is_traite = True
        evenement.save()
        
        logger.info(f"√âv√©nement externe analys√© avec succ√®s: {evenement_id}")
        
        return {
            'success': True,
            'evenement_id': evenement_id,
            'niveau_risque': evenement.niveau_risque,
            'zone_erosion': evenement.zone_erosion,
            'alertes_creees': AlerteEnrichie.objects.filter(
                donnees_contexte__contains={'evenement_id': evenement_id}
            ).count()
        }
        
    except EvenementExterne.DoesNotExist:
        logger.error(f"√âv√©nement externe introuvable: {evenement_id}")
        return {'success': False, 'error': '√âv√©nement introuvable'}
        
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse de l'√©v√©nement externe {evenement_id}: {e}")
        return {'success': False, 'error': str(e)}


@shared_task
def generer_rapport_evenements_externes_quotidien():
    """
    T√¢che pour g√©n√©rer un rapport quotidien des √©v√©nements externes
    """
    logger.info("üåä G√©n√©ration du rapport quotidien des √©v√©nements externes")
    
    try:
        aujourd_hui = timezone.now().date()
        hier = aujourd_hui - timedelta(days=1)
        
        # √âv√©nements d'hier
        evenements_hier = EvenementExterne.objects.filter(
            date_evenement__date=hier
        )
        
        # Statistiques par type
        stats_par_type = {}
        for type_choice in EvenementExterne.TYPE_CHOICES:
            type_code = type_choice[0]
            count = evenements_hier.filter(type_evenement=type_code).count()
            if count > 0:
                stats_par_type[type_code] = count
        
        # Statistiques par niveau de risque
        stats_par_risque = {}
        for risque_choice in EvenementExterne.NIVEAU_RISQUE_CHOICES:
            risque_code = risque_choice[0]
            count = evenements_hier.filter(niveau_risque=risque_code).count()
            if count > 0:
                stats_par_risque[risque_code] = count
        
        # √âv√©nements critiques
        evenements_critiques = evenements_hier.filter(niveau_risque='critique')
        
        # Alertes g√©n√©r√©es
        alertes_hier = AlerteEnrichie.objects.filter(
            date_creation__date=hier,
            donnees_contexte__contains={'type_source': 'evenement_externe'}
        )
        
        rapport = {
            'date': hier.isoformat(),
            'evenements': {
                'total': evenements_hier.count(),
                'par_type': stats_par_type,
                'par_niveau_risque': stats_par_risque,
                'critiques': evenements_critiques.count(),
                'necessitant_alerte': evenements_hier.filter(niveau_risque__in=['eleve', 'critique']).count()
            },
            'alertes': {
                'total': alertes_hier.count(),
                'actives': alertes_hier.filter(est_active=True).count(),
                'resolues': alertes_hier.filter(est_resolue=True).count(),
                'critiques': alertes_hier.filter(niveau='critique').count()
            },
            'sources': list(evenements_hier.values_list('source', flat=True).distinct()),
            'zones_erosion': list(evenements_hier.values_list('zone_erosion', flat=True).distinct()),
            'statut_systeme': 'op√©rationnel'
        }
        
        logger.info(f"Rapport quotidien √©v√©nements externes g√©n√©r√©: {rapport}")
        return rapport
        
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration du rapport quotidien √©v√©nements externes: {e}")
        return {'erreur': str(e)}