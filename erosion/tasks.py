from celery import shared_task
from django.utils import timezone
from datetime import timedelta, datetime
import random
import logging
from .models import (
    Capteur, Mesure, Zone, DonneesEnvironnementales, AnalyseErosion,
    CapteurArduino, MesureArduino, DonneesManquantes, LogCapteurArduino,
    EvenementExterne, FusionDonnees, PredictionEnrichie, AlerteEnrichie, ArchiveDonnees
)
from .services import DataConsolidationService
# Imports supprim√©s - fichiers de services inutilis√©s supprim√©s
from .services.analyse_fusion_service import AnalyseFusionService, ArchiveService

logger = logging.getLogger(__name__)


@shared_task
def generer_mesures_automatiques():
    """
    T√¢che Celery pour g√©n√©rer automatiquement des mesures
    Ex√©cut√©e toutes les X minutes selon la fr√©quence des capteurs
    """
    print("üîÑ G√©n√©ration automatique de mesures...")
    
    capteurs_actifs = Capteur.objects.filter(etat='actif')
    mesures_creees = 0
    
    for capteur in capteurs_actifs:
        # V√©rifier si le capteur doit prendre une mesure maintenant
        derniere_mesure = capteur.mesures.first()
        if derniere_mesure:
            temps_ecoule = timezone.now() - derniere_mesure.timestamp
            frequence_minutes = timedelta(minutes=capteur.frequence_mesure_min)
            
            if temps_ecoule < frequence_minutes:
                continue  # Pas encore le moment de prendre une mesure
        
        # G√©n√©rer une nouvelle mesure
        valeur = generer_valeur_mesure(capteur.type)
        unite = get_unite_mesure(capteur.type)
        
        Mesure.objects.create(
            capteur=capteur,
            valeur=valeur,
            unite=unite,
            timestamp=timezone.now(),
            qualite_donnee='bonne',
            commentaires="Mesure automatique g√©n√©r√©e"
        )
        mesures_creees += 1
    
    print(f"‚úÖ {mesures_creees} mesures g√©n√©r√©es automatiquement")
    return f"{mesures_creees} mesures cr√©√©es"


def generer_valeur_mesure(type_capteur):
    """G√©n√®re une valeur r√©aliste selon le type de capteur"""
    if type_capteur == "temperature":
        return round(random.uniform(24, 34), 1)
    elif type_capteur == "salinite":
        return round(random.uniform(30, 40), 2)
    elif type_capteur == "houle":
        return round(random.uniform(0.5, 3.5), 2)
    elif type_capteur == "vent":
        return round(random.uniform(5, 60), 1)
    elif type_capteur == "pluviometrie":
        return round(random.uniform(0, 100), 1)
    elif type_capteur == "niveau_mer":
        return round(random.uniform(-2, 4), 2)
    elif type_capteur == "ph":
        return round(random.uniform(7.5, 8.5), 2)
    elif type_capteur == "turbidite":
        return round(random.uniform(0.1, 50), 1)
    else:
        return round(random.uniform(0, 100), 2)


def get_unite_mesure(type_capteur):
    """Retourne l'unit√© de mesure selon le type de capteur"""
    unites = {
        'temperature': '¬∞C',
        'salinite': 'PSU',
        'houle': 'm',
        'vent': 'km/h',
        'pluviometrie': 'mm',
        'niveau_mer': 'm',
        'ph': 'pH',
        'turbidite': 'NTU'
    }
    return unites.get(type_capteur, 'unit')


@shared_task
def nettoyer_anciennes_mesures():
    """
    T√¢che pour nettoyer les anciennes mesures (plus de 1 an)
    """
    print("üßπ Nettoyage des anciennes mesures...")
    
    date_limite = timezone.now() - timedelta(days=365)
    anciennes_mesures = Mesure.objects.filter(timestamp__lt=date_limite)
    nombre_supprimees = anciennes_mesures.count()
    
    anciennes_mesures.delete()
    
    print(f"‚úÖ {nombre_supprimees} anciennes mesures supprim√©es")
    return f"{nombre_supprimees} mesures supprim√©es"


@shared_task
def verifier_etat_capteurs():
    """
    T√¢che pour v√©rifier l'√©tat des capteurs et g√©n√©rer des alertes
    """
    print("üîç V√©rification de l'√©tat des capteurs...")
    
    capteurs_defaillants = []
    
    for capteur in Capteur.objects.filter(etat='actif'):
        # V√©rifier si le capteur n'a pas envoy√© de donn√©es r√©cemment
        derniere_mesure = capteur.mesures.first()
        if derniere_mesure:
            temps_ecoule = timezone.now() - derniere_mesure.timestamp
            # Si pas de mesure depuis plus de 2x la fr√©quence normale
            frequence_max = timedelta(minutes=capteur.frequence_mesure_min * 2)
            
            if temps_ecoule > frequence_max:
                capteur.etat = 'defaillant'
                capteur.save()
                capteurs_defaillants.append(capteur.nom)
    
    print(f"‚ö†Ô∏è {len(capteurs_defaillants)} capteurs marqu√©s comme d√©faillants")
    return f"{len(capteurs_defaillants)} capteurs d√©faillants d√©tect√©s"


# ============================================================================
# NOUVELLES T√ÇCHES POUR LES FONCTIONNALIT√âS ENRICHIES
# ============================================================================

@shared_task
def collecter_donnees_environnementales():
    """
    T√¢che pour collecter automatiquement les donn√©es environnementales
    de toutes les zones actives
    """
    print("üåç Collecte automatique des donn√©es environnementales...")
    
    zones_actives = Zone.objects.all()
    donnees_collectees = 0
    
    for zone in zones_actives:
        try:
            # D√©finir la p√©riode de collecte (derni√®res 24h)
            end_date = datetime.now()
            start_date = end_date - timedelta(days=1)
            
            # Formater les dates pour les APIs
            start_date_str = start_date.strftime('%Y-%m-%d')
            end_date_str = end_date.strftime('%Y-%m-%d')
            
            # Collecter les donn√©es
            consolidation_service = DataConsolidationService()
            consolidated_data = consolidation_service.collect_all_data(
                zone, start_date_str, end_date_str
            )
            
            # Sauvegarder les donn√©es
            donnees_env = consolidation_service.save_consolidated_data(zone, consolidated_data)
            donnees_collectees += 1
            
            print(f"‚úÖ Donn√©es collect√©es pour {zone.nom}")
            
        except Exception as e:
            print(f"‚ùå Erreur collecte {zone.nom}: {e}")
    
    print(f"üìä {donnees_collectees} zones trait√©es")
    return f"{donnees_collectees} zones trait√©es"


@shared_task
def generer_analyses_erosion_automatiques():
    """
    T√¢che pour g√©n√©rer automatiquement des analyses d'√©rosion
    pour toutes les zones avec des donn√©es environnementales r√©centes
    """
    print("üî¨ G√©n√©ration automatique d'analyses d'√©rosion...")
    
    # R√©cup√©rer les zones avec des donn√©es environnementales r√©centes
    zones_avec_donnees = Zone.objects.filter(
        donnees_environnementales__date_collecte__gte=timezone.now() - timedelta(days=1)
    ).distinct()
    
    analyses_creees = 0
    
    for zone in zones_avec_donnees:
        try:
            # R√©cup√©rer les donn√©es environnementales les plus r√©centes
            donnees_env = DonneesEnvironnementales.objects.filter(
                zone=zone
            ).order_by('-date_collecte').first()
            
            if donnees_env:
                # V√©rifier si une analyse r√©cente existe d√©j√†
                analyse_recente = AnalyseErosion.objects.filter(
                    zone=zone,
                    donnees_environnementales=donnees_env,
                    date_analyse__gte=timezone.now() - timedelta(hours=6)
                ).exists()
                
                if not analyse_recente:
                    # Cr√©er une nouvelle analyse
                    analyse_view = AnalyseErosionViewSet()
                    analyse = analyse_view._calculer_analyse_erosion(zone, donnees_env, 30)
                    analyses_creees += 1
                    
                    print(f"‚úÖ Analyse cr√©√©e pour {zone.nom}")
            
        except Exception as e:
            print(f"‚ùå Erreur analyse {zone.nom}: {e}")
    
    print(f"üìà {analyses_creees} analyses cr√©√©es")
    return f"{analyses_creees} analyses cr√©√©es"


@shared_task
def nettoyer_donnees_anciennes():
    """
    T√¢che pour nettoyer les anciennes donn√©es environnementales et analyses
    """
    print("üßπ Nettoyage des anciennes donn√©es...")
    
    # Nettoyer les donn√©es environnementales anciennes (plus de 3 mois)
    date_limite_env = timezone.now() - timedelta(days=90)
    anciennes_donnees_env = DonneesEnvironnementales.objects.filter(
        date_collecte__lt=date_limite_env
    )
    nb_env_supprimees = anciennes_donnees_env.count()
    anciennes_donnees_env.delete()
    
    # Nettoyer les analyses anciennes (plus de 6 mois)
    date_limite_analyses = timezone.now() - timedelta(days=180)
    anciennes_analyses = AnalyseErosion.objects.filter(
        date_analyse__lt=date_limite_analyses
    )
    nb_analyses_supprimees = anciennes_analyses.count()
    anciennes_analyses.delete()
    
    print(f"‚úÖ {nb_env_supprimees} donn√©es environnementales supprim√©es")
    print(f"‚úÖ {nb_analyses_supprimees} analyses supprim√©es")
    
    return f"{nb_env_supprimees} donn√©es env et {nb_analyses_supprimees} analyses supprim√©es"


@shared_task
def synchroniser_donnees_cartographiques():
    """
    T√¢che pour synchroniser les donn√©es cartographiques avec les APIs externes
    """
    print("üó∫Ô∏è Synchronisation des donn√©es cartographiques...")
    
    # Cette t√¢che pourrait √™tre √©tendue pour t√©l√©charger automatiquement
    # les nouvelles images satellites, donn√©es de substrat, etc.
    
    zones_actives = Zone.objects.all()
    donnees_synchronisees = 0
    
    for zone in zones_actives:
        try:
            # Ici, on pourrait impl√©menter la logique pour :
            # - T√©l√©charger les nouvelles images satellites
            # - Mettre √† jour les donn√©es de substrat
            # - Synchroniser les donn√©es hydrographiques
            
            print(f"‚úÖ Donn√©es cartographiques synchronis√©es pour {zone.nom}")
            donnees_synchronisees += 1
            
        except Exception as e:
            print(f"‚ùå Erreur synchronisation {zone.nom}: {e}")
    
    print(f"üó∫Ô∏è {donnees_synchronisees} zones synchronis√©es")
    return f"{donnees_synchronisees} zones synchronis√©es"


@shared_task
def generer_rapport_quotidien():
    """
    T√¢che pour g√©n√©rer un rapport quotidien des activit√©s du syst√®me
    """
    print("üìä G√©n√©ration du rapport quotidien...")
    
    # Statistiques du jour
    aujourd_hui = timezone.now().date()
    
    # Nombre de mesures g√©n√©r√©es
    mesures_aujourd_hui = Mesure.objects.filter(
        timestamp__date=aujourd_hui
    ).count()
    
    # Nombre de donn√©es environnementales collect√©es
    donnees_env_aujourd_hui = DonneesEnvironnementales.objects.filter(
        date_collecte__date=aujourd_hui
    ).count()
    
    # Nombre d'analyses cr√©√©es
    analyses_aujourd_hui = AnalyseErosion.objects.filter(
        date_analyse__date=aujourd_hui
    ).count()
    
    # Zones actives
    zones_actives = Zone.objects.count()
    
    rapport = {
        'date': aujourd_hui.isoformat(),
        'mesures_generes': mesures_aujourd_hui,
        'donnees_environnementales': donnees_env_aujourd_hui,
        'analyses_erosion': analyses_aujourd_hui,
        'zones_actives': zones_actives,
        'statut_systeme': 'op√©rationnel'
    }
    
    print(f"üìà Rapport quotidien g√©n√©r√©: {rapport}")
    return rapport


# ============================================================================
# NOUVELLES T√ÇCHES POUR LES CAPTEURS ARDUINO
# ============================================================================

# T√¢ches supprim√©es - services Arduino inutilis√©s supprim√©s
# @shared_task
# def monitorer_capteurs_arduino():
#     """T√¢che supprim√©e - service ArduinoMonitoringService supprim√©"""
#     pass

# @shared_task
# def detecter_donnees_manquantes_arduino():
#     """T√¢che supprim√©e - service DataCompletionService supprim√©"""
#     pass

# @shared_task
# def completer_donnees_manquantes_arduino():
#     """T√¢che supprim√©e - service DataCompletionService supprim√©"""
#     pass

# @shared_task
# def simuler_donnees_arduino_test():
#     """T√¢che supprim√©e - service ArduinoConnectionService supprim√©"""
#     pass

# @shared_task
# def detecter_capteurs_automatique():
#     """T√¢che supprim√©e - service CapteurDetectionService supprim√©"""
#     pass

# @shared_task
# def envoyer_notifications_quotidiennes():
#     """T√¢che supprim√©e - service CapteurNotificationService supprim√©"""
#     pass


# ============================================================================
# NOUVELLES T√ÇCHES POUR √âV√âNEMENTS EXTERNES ET FUSION DE DONN√âES
# ============================================================================

@shared_task
def analyser_fusion_evenement(evenement_id: int):
    """
    T√¢che pour analyser un √©v√©nement externe et cr√©er une fusion de donn√©es
    """
    logger.info(f"Analyse de fusion pour l'√©v√©nement {evenement_id}")
    
    try:
        service = AnalyseFusionService()
        resultat = service.analyser_evenement(evenement_id)
        
        if resultat['success']:
            logger.info(f"Analyse termin√©e pour l'√©v√©nement {evenement_id}: {resultat['message']}")
            return f"Analyse r√©ussie: {resultat['message']}"
        else:
            logger.error(f"√âchec analyse √©v√©nement {evenement_id}: {resultat['message']}")
            return f"√âchec: {resultat['message']}"
            
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse de l'√©v√©nement {evenement_id}: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def analyser_fusion_zone(zone_id: int, periode_jours: int = 30):
    """
    T√¢che pour analyser une zone compl√®te et cr√©er des fusions de donn√©es
    """
    logger.info(f"Analyse de fusion pour la zone {zone_id} sur {periode_jours} jours")
    
    try:
        service = AnalyseFusionService()
        resultat = service.analyser_zone(zone_id, periode_jours)
        
        if resultat['success']:
            logger.info(f"Analyse de zone termin√©e: {resultat['message']}")
            return f"Analyse r√©ussie: {resultat['message']}"
        else:
            logger.error(f"√âchec analyse zone {zone_id}: {resultat['message']}")
            return f"√âchec: {resultat['message']}"
            
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse de la zone {zone_id}: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def traiter_evenements_en_attente():
    """
    T√¢che pour traiter les √©v√©nements externes en attente de traitement
    """
    logger.info("Traitement des √©v√©nements en attente")
    
    try:
        # R√©cup√©rer les √©v√©nements non trait√©s des derni√®res 24h
        date_limite = timezone.now() - timedelta(hours=24)
        evenements_en_attente = EvenementExterne.objects.filter(
            is_traite=False,
            is_valide=True,
            date_evenement__gte=date_limite
        ).order_by('date_evenement')
        
        evenements_traites = 0
        
        for evenement in evenements_en_attente:
            try:
                # Marquer comme trait√© et d√©clencher l'analyse
                evenement.is_traite = True
                evenement.save()
                
                # D√©clencher l'analyse
                analyser_fusion_evenement.delay(evenement.id)
                evenements_traites += 1
                
            except Exception as e:
                logger.error(f"Erreur traitement √©v√©nement {evenement.id}: {e}")
        
        logger.info(f"Traitement termin√©: {evenements_traites} √©v√©nements trait√©s")
        return f"{evenements_traites} √©v√©nements trait√©s"
        
    except Exception as e:
        logger.error(f"Erreur lors du traitement des √©v√©nements en attente: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def nettoyer_anciens_evenements():
    """
    T√¢che pour nettoyer les anciens √©v√©nements externes
    """
    logger.info("Nettoyage des anciens √©v√©nements externes")
    
    try:
        # Supprimer les √©v√©nements de simulation de plus de 30 jours
        date_limite_simulation = timezone.now() - timedelta(days=30)
        anciens_simulations = EvenementExterne.objects.filter(
            is_simulation=True,
            date_evenement__lt=date_limite_simulation
        )
        nb_simulations_supprimees = anciens_simulations.count()
        anciens_simulations.delete()
        
        # Supprimer les √©v√©nements invalides de plus de 7 jours
        date_limite_invalides = timezone.now() - timedelta(days=7)
        anciens_invalides = EvenementExterne.objects.filter(
            is_valide=False,
            date_evenement__lt=date_limite_invalides
        )
        nb_invalides_supprimees = anciens_invalides.count()
        anciens_invalides.delete()
        
        # Supprimer les √©v√©nements trait√©s de plus de 90 jours
        date_limite_traites = timezone.now() - timedelta(days=90)
        anciens_traites = EvenementExterne.objects.filter(
            is_traite=True,
            date_evenement__lt=date_limite_traites
        )
        nb_traites_supprimees = anciens_traites.count()
        anciens_traites.delete()
        
        logger.info(f"Nettoyage termin√©: {nb_simulations_supprimees} simulations, "
                   f"{nb_invalides_supprimees} invalides, {nb_traites_supprimees} trait√©s supprim√©s")
        
        return f"{nb_simulations_supprimees + nb_invalides_supprimees + nb_traites_supprimees} √©v√©nements supprim√©s"
        
    except Exception as e:
        logger.error(f"Erreur lors du nettoyage des √©v√©nements: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def nettoyer_anciennes_fusions():
    """
    T√¢che pour nettoyer les anciennes fusions de donn√©es
    """
    logger.info("Nettoyage des anciennes fusions de donn√©es")
    
    try:
        # Supprimer les fusions termin√©es de plus de 6 mois
        date_limite = timezone.now() - timedelta(days=180)
        anciennes_fusions = FusionDonnees.objects.filter(
            statut='terminee',
            date_creation__lt=date_limite
        )
        nb_fusions_supprimees = anciennes_fusions.count()
        anciennes_fusions.delete()
        
        # Supprimer les fusions en erreur de plus de 30 jours
        date_limite_erreurs = timezone.now() - timedelta(days=30)
        anciennes_erreurs = FusionDonnees.objects.filter(
            statut='erreur',
            date_creation__lt=date_limite_erreurs
        )
        nb_erreurs_supprimees = anciennes_erreurs.count()
        anciennes_erreurs.delete()
        
        logger.info(f"Nettoyage fusions termin√©: {nb_fusions_supprimees} fusions, "
                   f"{nb_erreurs_supprimees} erreurs supprim√©es")
        
        return f"{nb_fusions_supprimees + nb_erreurs_supprimees} fusions supprim√©es"
        
    except Exception as e:
        logger.error(f"Erreur lors du nettoyage des fusions: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def nettoyer_anciennes_predictions():
    """
    T√¢che pour nettoyer les anciennes pr√©dictions enrichies
    """
    logger.info("Nettoyage des anciennes pr√©dictions enrichies")
    
    try:
        # Supprimer les pr√©dictions de plus de 1 an
        date_limite = timezone.now() - timedelta(days=365)
        anciennes_predictions = PredictionEnrichie.objects.filter(
            date_prediction__lt=date_limite
        )
        nb_predictions_supprimees = anciennes_predictions.count()
        anciennes_predictions.delete()
        
        logger.info(f"Nettoyage pr√©dictions termin√©: {nb_predictions_supprimees} pr√©dictions supprim√©es")
        return f"{nb_predictions_supprimees} pr√©dictions supprim√©es"
        
    except Exception as e:
        logger.error(f"Erreur lors du nettoyage des pr√©dictions: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def nettoyer_anciennes_alertes():
    """
    T√¢che pour nettoyer les anciennes alertes enrichies
    """
    logger.info("Nettoyage des anciennes alertes enrichies")
    
    try:
        # R√©soudre les alertes anciennes non r√©solues
        date_limite_resolution = timezone.now() - timedelta(days=30)
        alertes_anciennes = AlerteEnrichie.objects.filter(
            est_resolue=False,
            date_creation__lt=date_limite_resolution
        )
        nb_alertes_resolues = alertes_anciennes.count()
        
        for alerte in alertes_anciennes:
            alerte.est_resolue = True
            alerte.est_active = False
            alerte.date_resolution = timezone.now()
            alerte.commentaires += " R√©solue automatiquement par nettoyage."
            alerte.save()
        
        # Supprimer les alertes r√©solues de plus de 6 mois
        date_limite_suppression = timezone.now() - timedelta(days=180)
        anciennes_alertes = AlerteEnrichie.objects.filter(
            est_resolue=True,
            date_resolution__lt=date_limite_suppression
        )
        nb_alertes_supprimees = anciennes_alertes.count()
        anciennes_alertes.delete()
        
        logger.info(f"Nettoyage alertes termin√©: {nb_alertes_resolues} r√©solues, "
                   f"{nb_alertes_supprimees} supprim√©es")
        
        return f"{nb_alertes_resolues} alertes r√©solues, {nb_alertes_supprimees} supprim√©es"
        
    except Exception as e:
        logger.error(f"Erreur lors du nettoyage des alertes: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def creer_archive_donnees(type_donnees: str, zone_id: int, periode_jours: int):
    """
    T√¢che pour cr√©er une archive de donn√©es
    """
    logger.info(f"Cr√©ation d'archive {type_donnees} pour la zone {zone_id}")
    
    try:
        service = ArchiveService()
        resultat = service.creer_archive(type_donnees, zone_id, periode_jours)
        
        if resultat['success']:
            logger.info(f"Archive cr√©√©e: {resultat['archive_id']}")
            return f"Archive cr√©√©e: {resultat['nombre_elements']} √©l√©ments"
        else:
            logger.error(f"√âchec cr√©ation archive: {resultat['message']}")
            return f"√âchec: {resultat['message']}"
            
    except Exception as e:
        logger.error(f"Erreur lors de la cr√©ation de l'archive: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def purger_anciennes_archives(periode_jours: int = 365):
    """
    T√¢che pour purger les anciennes archives
    """
    logger.info(f"Purge des archives de plus de {periode_jours} jours")
    
    try:
        date_limite = timezone.now() - timedelta(days=periode_jours)
        anciennes_archives = ArchiveDonnees.objects.filter(
            date_archivage__lt=date_limite,
            est_disponible=True
        )
        
        nb_archives_supprimees = 0
        
        for archive in anciennes_archives:
            try:
                # Supprimer le fichier physique
                import os
                if os.path.exists(archive.chemin_fichier):
                    os.remove(archive.chemin_fichier)
                
                # Marquer comme supprim√©e
                archive.est_disponible = False
                archive.date_suppression = timezone.now()
                archive.save()
                
                nb_archives_supprimees += 1
                
            except Exception as e:
                logger.error(f"Erreur suppression archive {archive.id}: {e}")
        
        logger.info(f"Purge termin√©e: {nb_archives_supprimees} archives supprim√©es")
        return f"{nb_archives_supprimees} archives supprim√©es"
        
    except Exception as e:
        logger.error(f"Erreur lors de la purge des archives: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def generer_rapport_fusion_quotidien():
    """
    T√¢che pour g√©n√©rer un rapport quotidien de fusion des donn√©es
    """
    logger.info("G√©n√©ration du rapport quotidien de fusion")
    
    try:
        aujourd_hui = timezone.now().date()
        hier = aujourd_hui - timedelta(days=1)
        
        # Statistiques des √©v√©nements
        evenements_hier = EvenementExterne.objects.filter(
            date_evenement__date=hier
        )
        
        # Statistiques des fusions
        fusions_hier = FusionDonnees.objects.filter(
            date_creation__date=hier
        )
        
        # Statistiques des pr√©dictions
        predictions_hier = PredictionEnrichie.objects.filter(
            date_prediction__date=hier
        )
        
        # Statistiques des alertes
        alertes_hier = AlerteEnrichie.objects.filter(
            date_creation__date=hier
        )
        
        rapport = {
            'date': hier.isoformat(),
            'evenements': {
                'total': evenements_hier.count(),
                'traites': evenements_hier.filter(is_traite=True).count(),
                'non_traites': evenements_hier.filter(is_traite=False).count(),
                'simulations': evenements_hier.filter(is_simulation=True).count(),
                'par_type': dict(evenements_hier.values_list('type_evenement').annotate(count=Count('id')))
            },
            'fusions': {
                'total': fusions_hier.count(),
                'terminees': fusions_hier.filter(statut='terminee').count(),
                'en_cours': fusions_hier.filter(statut='en_cours').count(),
                'erreurs': fusions_hier.filter(statut='erreur').count()
            },
            'predictions': {
                'total': predictions_hier.count(),
                'erosion_predite': predictions_hier.filter(erosion_predite=True).count(),
                'erosion_non_predite': predictions_hier.filter(erosion_predite=False).count(),
                'par_niveau': dict(predictions_hier.values_list('niveau_erosion').annotate(count=Count('id')))
            },
            'alertes': {
                'total': alertes_hier.count(),
                'actives': alertes_hier.filter(est_active=True).count(),
                'resolues': alertes_hier.filter(est_resolue=True).count(),
                'par_niveau': dict(alertes_hier.values_list('niveau').annotate(count=Count('id')))
            },
            'statut_systeme': 'op√©rationnel'
        }
        
        logger.info(f"Rapport quotidien g√©n√©r√©: {rapport}")
        return rapport
        
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration du rapport quotidien: {e}")
        return f"Erreur: {str(e)}"


@shared_task
def exporter_donnees_ia():
    """
    T√¢che pour exporter les donn√©es pour l'IA (format ML)
    """
    logger.info("Export des donn√©es pour l'IA")
    
    try:
        # R√©cup√©rer les donn√©es des 6 derniers mois
        date_limite = timezone.now() - timedelta(days=180)
        
        # Donn√©es d'entra√Ænement
        donnees_entrainement = {
            'evenements': [],
            'mesures_arduino': [],
            'fusions': [],
            'predictions': []
        }
        
        # √âv√©nements
        evenements = EvenementExterne.objects.filter(
            date_evenement__gte=date_limite,
            is_valide=True
        )
        
        for e in evenements:
            donnees_entrainement['evenements'].append({
                'type_evenement': e.type_evenement,
                'intensite': e.intensite,
                'zone_id': e.zone.id,
                'date_evenement': e.date_evenement.isoformat(),
                'duree_minutes': e.duree_minutes or 0,
                'rayon_impact_km': e.rayon_impact_km or 0
            })
        
        # Mesures Arduino
        mesures = MesureArduino.objects.filter(
            timestamp__gte=date_limite,
            est_valide=True
        ).select_related('capteur')
        
        for m in mesures:
            donnees_entrainement['mesures_arduino'].append({
                'capteur_type': m.capteur.type_capteur,
                'valeur': m.valeur,
                'zone_id': m.capteur.zone.id,
                'timestamp': m.timestamp.isoformat(),
                'qualite_donnee': m.qualite_donnee
            })
        
        # Fusions
        fusions = FusionDonnees.objects.filter(
            date_creation__gte=date_limite,
            statut='terminee'
        )
        
        for f in fusions:
            donnees_entrainement['fusions'].append({
                'zone_id': f.zone.id,
                'score_erosion': f.score_erosion,
                'probabilite_erosion': f.probabilite_erosion,
                'mesures_count': f.mesures_arduino_count,
                'evenements_count': f.evenements_externes_count,
                'facteurs_dominants': f.facteurs_dominants
            })
        
        # Pr√©dictions
        predictions = PredictionEnrichie.objects.filter(
            date_prediction__gte=date_limite
        )
        
        for p in predictions:
            donnees_entrainement['predictions'].append({
                'zone_id': p.zone.id,
                'erosion_predite': p.erosion_predite,
                'niveau_erosion': p.niveau_erosion,
                'confiance_pourcentage': p.confiance_pourcentage,
                'taux_erosion_pred': p.taux_erosion_pred_m_an,
                'horizon_jours': p.horizon_jours
            })
        
        # Sauvegarder le fichier d'export
        import json
        import os
        
        chemin_export = f"exports/ia/donnees_entrainement_{timezone.now().strftime('%Y%m%d')}.json"
        os.makedirs(os.path.dirname(chemin_export), exist_ok=True)
        
        with open(chemin_export, 'w', encoding='utf-8') as f:
            json.dump(donnees_entrainement, f, ensure_ascii=False, indent=2)
        
        # Statistiques
        stats = {
            'evenements': len(donnees_entrainement['evenements']),
            'mesures': len(donnees_entrainement['mesures_arduino']),
            'fusions': len(donnees_entrainement['fusions']),
            'predictions': len(donnees_entrainement['predictions']),
            'chemin_fichier': chemin_export,
            'date_export': timezone.now().isoformat()
        }
        
        logger.info(f"Export IA termin√©: {stats}")
        return f"Export r√©ussi: {stats['evenements']} √©v√©nements, {stats['mesures']} mesures, {stats['fusions']} fusions, {stats['predictions']} pr√©dictions"
        
    except Exception as e:
        logger.error(f"Erreur lors de l'export IA: {e}")
        return f"Erreur: {str(e)}"
